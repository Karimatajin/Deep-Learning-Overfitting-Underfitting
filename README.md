# Overfitting in Deep Learning

- `Overfitting` is "the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably". 

- `An overfitted model` is a statistical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure.
![image](https://user-images.githubusercontent.com/43942029/82112978-3b6d7c00-9720-11ea-8604-cec4572ecbc8.png)

![image](https://user-images.githubusercontent.com/43942029/82112980-40323000-9720-11ea-811d-00f618ab72a8.png)
## Handling Overfitting with Dropout

- `Dropout` is introduced to overcome overfitting problem in neural networks. This technique proposes to drop nodes randomly during training to simplify the neural network as it is moving towards its optimum weights and biases.
